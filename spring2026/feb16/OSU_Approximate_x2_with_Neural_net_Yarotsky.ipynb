{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ev2bATxe1OsE"
   },
   "outputs": [],
   "source": [
    "# Create a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYR1TljZrYg1"
   },
   "source": [
    "# Algorithm 4: A one hidden layer dense NN initialised by the formula :\n",
    "\n",
    "$$ f_N(x) = \\sum_{|i|\\leq N} (f^{obj})''(ih)R(x-ih)h $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ofy-DkKmrN1a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "  return 1-np.cos(2*np.pi*x)\n",
    "\n",
    "def f_2(x):\n",
    "  return (2*np.pi)**2*np.cos(2*np.pi*x)\n",
    "\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, N, f, f_2):\n",
    "        # we don't actually need to pass f -- we could, if we do automatic differentiation to get f''\n",
    "        super(Net4, self).__init__()\n",
    "        self.N = N\n",
    "        self.dx = 1./N\n",
    "        self.f = f\n",
    "        self.f_2 = f_2\n",
    "\n",
    "        self.fc1 = nn.Linear(nInput, self.N)\n",
    "\n",
    "        # We know the exact structure of the neural network, following the formula\n",
    "        # So overwrite the weights with the right values\n",
    "        with torch.no_grad():\n",
    "          self.fc1.weight.copy_(self.init_W0(self.N))\n",
    "          self.fc1.bias.copy_(self.init_b0(self.N))\n",
    "\n",
    "        self.fc2 = nn.Linear(self.N, nOutput, bias=False)\n",
    "        with torch.no_grad():\n",
    "          self.fc2.weight.copy_(self.init_W1(self.N,self.dx))\n",
    "\n",
    "        self.Relu = nn.ReLU()\n",
    "\n",
    "    def init_W0(self,N):\n",
    "      return torch.ones(N,1)\n",
    "\n",
    "    def init_b0(self,N):\n",
    "      return torch.tensor(-np.linspace(0,1,N))\n",
    "\n",
    "    def init_W1(self,N,dx):\n",
    "      array3 = np.zeros((1,N))\n",
    "      for i in range(N):\n",
    "        array3[0,i] = self.f_2(i*dx)*dx\n",
    "\n",
    "      return torch.tensor(array3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "axj3Xki1rzsv",
    "outputId": "52fc195c-999a-4b14-d2ee-6762ee657145"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N = 1000\n",
    "dx = 1./N\n",
    "\n",
    "net = Net4(1,1,N,f,f_2)\n",
    "\n",
    "x_p = torch.tensor(np.reshape(np.linspace(0,1,N),(N,1)),dtype=torch.float)\n",
    "y_p = net.forward(x_p)\n",
    "\n",
    "plt.plot(x_p.detach().numpy(),y_p.detach().numpy(),label='approx')\n",
    "plt.plot(x_p.detach().numpy(),1-np.cos(2*np.pi*x_p.detach().numpy()),label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "e6zVkQnKzllx",
    "outputId": "47b03e6e-6485-4c90-842c-e8ea59646f27"
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "dx = 1./N\n",
    "\n",
    "def x2(x):\n",
    "  return x**2\n",
    "\n",
    "def x2_2(x):\n",
    "  return 2.\n",
    "\n",
    "net = Net4(1,1,N,x2,x2_2) # the way the net is written is for interval [0,1]\n",
    "\n",
    "x_p = torch.tensor(np.reshape(np.linspace(0,1,N),(N,1)),dtype=torch.float)\n",
    "y_p = net.forward(x_p)\n",
    "\n",
    "plt.plot(x_p.detach().numpy(),y_p.detach().numpy(),label='approx')\n",
    "plt.plot(x_p.detach().numpy(),x_p.detach().numpy()**2,label='true')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWbGNew105KX"
   },
   "source": [
    "# Algorithm 6: Implementation of $x\\to x^2$ using the architecture (1,3,...,3,1) with p hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "heQFygAU0tdP",
    "outputId": "f2958518-a9ff-41e5-e186-557ebf53f663"
   },
   "outputs": [],
   "source": [
    "fac = 4\n",
    "depth = 2\n",
    "\n",
    "class Net6(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, fac, depth):\n",
    "        super(Net6, self).__init__()\n",
    "        self.fac=fac\n",
    "        self.depth = depth\n",
    "\n",
    "        self.fc1 = nn.Linear(nInput, 3)\n",
    "        with torch.no_grad():\n",
    "          self.fc1.weight.copy_(self.init_W0())\n",
    "          self.fc1.bias.copy_(self.init_b())\n",
    "\n",
    "        self.fc2 = nn.Linear(3,3)\n",
    "        with torch.no_grad():\n",
    "          self.fc2.weight.copy_(self.init_W1())\n",
    "          self.fc2.bias.copy_(self.init_b())\n",
    "\n",
    "        self.fc3 = nn.Linear(3,3)\n",
    "        with torch.no_grad():\n",
    "          self.fc3.weight.copy_(self.init_W2())\n",
    "          self.fc3.bias.copy_(self.init_b())\n",
    "\n",
    "        self.fc4 = nn.Linear(3,1,bias=False)\n",
    "        with torch.no_grad():\n",
    "          self.fc4.weight.copy_(self.init_W3())\n",
    "\n",
    "        self.Relu = nn.ReLU()\n",
    "\n",
    "    def init_W0(self):\n",
    "      return torch.tensor(np.reshape(np.array([1,1,0]),(3,1)))\n",
    "\n",
    "    def init_b(self):\n",
    "      return torch.tensor(np.array([0,-0.5,0]))\n",
    "\n",
    "    def init_W1(self):\n",
    "      return torch.tensor(np.transpose(np.array([[2,2,2/self.fac],[-4,-4,-4/self.fac],[0,0,1]])))\n",
    "\n",
    "    def init_W2(self):\n",
    "      return torch.tensor(np.transpose(np.array([[2,2,2/self.fac**2],[-4,-4,-4/self.fac**2],[0,0,1]])))\n",
    "\n",
    "    def init_W3(self):\n",
    "      return torch.tensor([2/self.fac**(self.depth+1),-4/self.fac**(self.depth+1),1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net6(1,1,fac,depth) # the way the net is written is for interval [0,1]\n",
    "N = 100\n",
    "x_p = torch.tensor(np.reshape(np.linspace(0,1,N),(N,1)),dtype=torch.float)\n",
    "y_p = net.forward(x_p)\n",
    "\n",
    "plt.plot(x_p.detach().numpy(),y_p.detach().numpy())\n",
    "plt.plot(x_p.detach().numpy(),x_p.detach().numpy()*(1-x_p.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "r942tVdvF_rg",
    "outputId": "54ae25ec-4c7f-454e-ace5-d6e94e6b166e"
   },
   "outputs": [],
   "source": [
    "# generalising to p layers\n",
    "\n",
    "class Net6(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, fac, depth):\n",
    "        super(Net6, self).__init__()\n",
    "        self.fac=fac\n",
    "        self.depth = depth\n",
    "\n",
    "        self.fc1 = nn.Linear(nInput, 3)\n",
    "        with torch.no_grad():\n",
    "          self.fc1.weight.copy_(self.init_W0())\n",
    "          self.fc1.bias.copy_(self.init_b())\n",
    "\n",
    "        self.hidden_layers = []\n",
    "\n",
    "        for i in range(self.depth):\n",
    "          self.hidden_layers.append(nn.Linear(3,3))\n",
    "          with torch.no_grad():\n",
    "            self.hidden_layers[i].weight.copy_(self.init_W_hidden(i+1))\n",
    "            self.hidden_layers[i].bias.copy_(self.init_b())\n",
    "\n",
    "        self.fc_last = nn.Linear(3,1,bias=False)\n",
    "        with torch.no_grad():\n",
    "          self.fc_last.weight.copy_(self.init_W_last())\n",
    "\n",
    "        self.Relu = nn.ReLU()\n",
    "\n",
    "    def init_W0(self):\n",
    "      return torch.tensor(np.reshape(np.array([1,1,0]),(3,1)))\n",
    "\n",
    "    def init_b(self):\n",
    "      return torch.tensor(np.array([0,-0.5,0]))\n",
    "\n",
    "    def init_W_hidden(self,r):\n",
    "      return torch.tensor(np.transpose(np.array([[2,2,2/self.fac**r],[-4,-4,-4/self.fac**r],[0,0,1]])))\n",
    "\n",
    "    def init_W_last(self):\n",
    "      return torch.tensor([2/self.fac**(self.depth+1),-4/self.fac**(self.depth+1),1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Relu(x)\n",
    "        for i in range(self.depth):\n",
    "          x = self.hidden_layers[i](x)\n",
    "          x = self.Relu(x)\n",
    "\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "errors = []\n",
    "for depth in [2,3,4,5,6,7,8]:\n",
    "  fac = 4\n",
    "\n",
    "  net = Net6(1,1,fac,depth) # the way the net is written is for interval [0,1]\n",
    "  N = 100\n",
    "  x_p = torch.tensor(np.reshape(np.linspace(0,1,N),(N,1)),dtype=torch.float)\n",
    "  y_p = net.forward(x_p)\n",
    "\n",
    "  plt.plot(x_p.detach().numpy(),y_p.detach().numpy(),marker='.')\n",
    "  plt.plot(x_p.detach().numpy(),x_p.detach().numpy()*(1-x_p.detach().numpy()))\n",
    "  errors.append(np.linalg.norm(y_p.detach().numpy().flatten()-x_p.detach().numpy().flatten()*(1-x_p.detach().numpy().flatten()),np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "lw1MaCWVf2Sw",
    "outputId": "a6620a44-dc2e-4712-9616-db41bd27035d"
   },
   "outputs": [],
   "source": [
    "true_error = [float(4**(-float(a))) for a in [2,3,4,5,6,7,8]]\n",
    "plt.plot([2,3,4,5,6,7,8],np.log(errors))\n",
    "plt.plot([2,3,4,5,6,7,8],np.log(true_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZPbYZCg-Fql"
   },
   "source": [
    "Do the same, but with a dataset and solving the minimisation problem:\n",
    "$$ J(w) = \\arg \\min_w \\sum_{i} \\ell(f_w(x_i),y_i)$$\n",
    "\n",
    "for $\\ell$ the mean squared error, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OkMK7usHJT3"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import CustomDataset\n",
    "\n",
    "nData = 500\n",
    "x_train = np.random.uniform(low=0.0, high=1, size=(nData,))\n",
    "y_train = x_train*(1-x_train) #1-np.cos(2*np.pi*x_train)\n",
    "\n",
    "x_test = np.sort(np.random.uniform(low=0.0, high=1.0, size=(50,)))\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#   def __init__(self, x,y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "\n",
    "#   def __len__(self):\n",
    "#         return len(self.x)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#         sample = {'x': self.x[idx], 'y': self.y[idx]}\n",
    "\n",
    "        # return sample\n",
    "\n",
    "training_data = CustomDataset(x_train,y_train)\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpnNcwSvshcK"
   },
   "outputs": [],
   "source": [
    "class Net_learn(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, depth, nhidden):\n",
    "        super(Net_learn, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.nhidden = nhidden\n",
    "\n",
    "        self.fc1 = nn.Linear(nInput, self.nhidden)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "\n",
    "        for i in range(self.depth):\n",
    "          self.hidden_layers.append(nn.Linear(self.nhidden,self.nhidden))\n",
    "\n",
    "        self.fc_last = nn.Linear(self.nhidden,1,bias=False)\n",
    "        self.Relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Relu(x)\n",
    "        for i in range(self.depth):\n",
    "          x = self.hidden_layers[i](x)\n",
    "          x = self.Relu(x)\n",
    "\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfO-fRHfH9w-"
   },
   "outputs": [],
   "source": [
    "model = Net_learn(1,1,4,100)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        data_size = data['x'].shape[0]\n",
    "        inputs = data['x'].reshape((data_size,1))\n",
    "        labels = data['y'].reshape((data_size,1))\n",
    "        inputs = torch.tensor(inputs.clone().detach(),dtype=torch.float32)\n",
    "        labels = torch.tensor(labels.clone().detach(),dtype=torch.float32)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "    #scheduler.step()\n",
    "    print(running_loss)\n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaz9zs5Lw7q5",
    "outputId": "e9a04ca3-1ace-4bdf-8130-301dc434a09f"
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "  train_one_epoch(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPx-xlj3suGQ"
   },
   "outputs": [],
   "source": [
    "y = model.forward(torch.reshape(torch.tensor(x_test,dtype=torch.float32),(50,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "dh5oesrftJeL",
    "outputId": "62930d18-7815-4401-d4fb-d1ef4909fbc1"
   },
   "outputs": [],
   "source": [
    "plt.plot(x_test, y.detach().numpy(),ls='None',marker='.')\n",
    "plt.plot(x_test, x_test*(1-x_test))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
