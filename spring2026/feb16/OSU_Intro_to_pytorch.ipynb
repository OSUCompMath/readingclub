{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra16-6-6owPC"
   },
   "outputs": [],
   "source": [
    "# We start by loading pytorch and looking at fundamental blocks of pytorch (tensors, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dM8jdXfe1KTw",
    "outputId": "b16346c8-5c8f-4cf9-96b2-8c5e12b749ee"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.empty(3, 4) #matrix\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "zeros = torch.zeros(2, 3, 10)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)\n",
    "\n",
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone() # returns a copy of the input\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone() # returns a new tensor, detached from the current graph (can't differentiate)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev2bATxe1OsE"
   },
   "source": [
    "# We generate a neural network\n",
    "\n",
    "Consider a scalar function $f(x)=cos(x)$. We want to approximate this using a neural network.\n",
    "\n",
    "What are the dimensions of the input and output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJGXpFY4o7ZS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # we need to instantiate the network and write a forward evaluation of it\n",
    "    # This object can be differentiated\n",
    "    def __init__(self, nInput, nOutput, n_hidden=3, n_layers=2):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_layer = nn.Linear(nInput, self.n_hidden)\n",
    "        self.output_layer = nn.Linear(self.n_hidden, nOutput)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        for i in range(n_layers):\n",
    "          self.hidden_layers.append(nn.Linear(self.n_hidden, self.n_hidden))\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n",
    "\n",
    "        # The values are initialized from uniformly sampling U(−k,k), where k = 1/in_features​.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn+linear#torch.nn.Linear\n",
    "\n",
    "        self.activation = nn.ReLU() # you can test different activation functions and layers\n",
    "                                    # https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        for layer in self.hidden_layers:\n",
    "          x = layer(x)\n",
    "          x = self.activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYLwOCY0C63W",
    "outputId": "5442e8a6-f85b-4b1d-f1fb-cd00347af928"
   },
   "outputs": [],
   "source": [
    "# Scalar problem\n",
    "nInput = 1\n",
    "nOutput = 1\n",
    "net = Net(nInput,nOutput,n_hidden=3)\n",
    "print(net)\n",
    "\n",
    "# generate an input\n",
    "input_tensor = torch.rand(1,nInput) # (nDataPoints, size input)\n",
    "print(f\"Input tensor: {input_tensor}\")\n",
    "print(f\"Network output: {net.forward(input_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U40P7ElSC--l",
    "outputId": "c276cbd1-03a6-4467-e20e-b670dc2f3314"
   },
   "outputs": [],
   "source": [
    "# Multi-dimensional problem\n",
    "nInput = 10\n",
    "nOutput = 10\n",
    "net = Net(nInput,nOutput)\n",
    "print(net)\n",
    "\n",
    "# generate an input\n",
    "input_tensor = torch.rand(1,nInput)\n",
    "print(f\"Input tensor: {input_tensor}\")\n",
    "print(f\"Network output: {net.forward(input_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-levBf3s5ER"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from datasets import CustomDataset\n",
    "\n",
    "import warnings # remove warnings (not nice :) )\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nData = 200\n",
    "x_train = np.random.uniform(low=0.0, high=1, size=(nData,))\n",
    "y_train = 1-np.cos(2*np.pi*x_train)\n",
    "\n",
    "x_test = np.sort(np.random.uniform(low=0.0, high=1.0, size=(50,)))\n",
    "batch_size = 64\n",
    "\n",
    "training_data = CustomDataset(x_train,y_train)\n",
    "training_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "def train_one_epoch(epoch_index,debug=1):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        data_shape = data['x'].shape[0]\n",
    "        inputs = data['x'].reshape((data_shape,1))\n",
    "        labels = data['y'].reshape((data_shape,1))\n",
    "        inputs = torch.tensor(inputs.clone().detach(),dtype=torch.float32)\n",
    "        labels = torch.tensor(labels.clone().detach(),dtype=torch.float32)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        if debug == 1:\n",
    "          if i == 0:\n",
    "            grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n",
    "            for key in grads.keys():\n",
    "              print(f\"{key}:{grads[key]}\")\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    #print(f\"Loss:{running_loss}\")\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vItmrYE-AQjG"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings # remove warnings (not nice :) )\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nData = 200\n",
    "x_train = np.random.uniform(low=0.0, high=1, size=(nData,))\n",
    "y_train = 1-np.cos(2*np.pi*x_train)\n",
    "\n",
    "x_test = np.sort(np.random.uniform(low=0.0, high=1.0, size=(50,)))\n",
    "batch_size = 64\n",
    "\n",
    "training_data = CustomDataset(x_train,y_train)\n",
    "training_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "def train_one_epoch(epoch_index,debug=1):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        data_shape = data['x'].shape[0]\n",
    "        inputs = data['x'].reshape((data_shape,1))\n",
    "        labels = data['y'].reshape((data_shape,1))\n",
    "        inputs = torch.tensor(inputs.clone().detach(),dtype=torch.float32)\n",
    "        labels = torch.tensor(labels.clone().detach(),dtype=torch.float32)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        if debug == 1:\n",
    "          if i == 0:\n",
    "            grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n",
    "            for key in grads.keys():\n",
    "              print(f\"{key}:{grads[key]}\")\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YhjL-PHs6AA"
   },
   "outputs": [],
   "source": [
    "def init_weights_zero(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "          m.weight.data.fill_(0.0)\n",
    "          m.bias.data.fill_(0.0)\n",
    "\n",
    "def init_weights_constant(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "          m.weight.data.fill_(1.0)\n",
    "          m.bias.data.fill_(1.0)\n",
    "\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(m.weight)\n",
    "          m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zoPmoMjs8Om",
    "outputId": "d53d0bd3-cb06-40ca-b514-d769d8871b0a"
   },
   "outputs": [],
   "source": [
    "for initialisation_type, initialisation in zip(['zeros','constant','xavier'],[init_weights_zero, init_weights_constant, init_weights_xavier]):\n",
    "  model = Net(1,1,n_hidden=3)\n",
    "  model.apply(initialisation)\n",
    "\n",
    "  loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "  print(f\"Initialisation type: {initialisation_type}\")\n",
    "  for i in range(10):\n",
    "    _ = train_one_epoch(i,debug=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu_EpijgtDml"
   },
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "tenhgc_rtCwh",
    "outputId": "0c435f37-90ef-465b-932c-509515f9bd2c"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "models = {}\n",
    "for optimizer_type in ['SGD','Adam']:\n",
    "  for lr in [0.1,0.01,0.001]:\n",
    "    losses = []\n",
    "    model = Net(1,1,n_hidden=50, n_layers=3)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    if optimizer_type == 'SGD':\n",
    "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_type == 'Adam':\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(20):\n",
    "      losses.append(train_one_epoch(i,debug=0))\n",
    "\n",
    "    plt.semilogy(losses,label=f'{optimizer_type} lr = {lr}',marker='.')\n",
    "    plt.legend()\n",
    "    models[f\"{optimizer_type}_{lr}\"] = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "Fujwj8ldbZzu",
    "outputId": "5a92f5cc-c882-40bf-92a5-c9c1ff7daf40"
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.linspace(0,1,10),models[f\"Adam_0.01\"](torch.linspace(0,1,10).unsqueeze(1)).detach().numpy())\n",
    "plt.plot(torch.linspace(0,1,10),1-np.cos(2*np.pi*torch.linspace(0,1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3wyg1EqtMB_"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a3ZbICPl52u",
    "outputId": "217cec8b-499a-4f84-f0af-6f1859f82fa3"
   },
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "## transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "## download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "NJt0Y36kl9oM",
    "outputId": "4ff8060b-f328-4b36-bd0c-f044795ee85d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-4-Ys66mBTj"
   },
   "outputs": [],
   "source": [
    "# set up CNNs\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # (in channels, out channels, kernel size) https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "        self.pool = nn.MaxPool2d(2, 2) # kernel_size\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120) # a bit nasty, should catch the size without hardcoding it...\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x)) # F.relu() seems to improve convergence (idk why)\n",
    "        x = self.pool(self.conv2(x)) # F.relu()\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7fXoVrxmB_z"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDNaWGuMmD2k",
    "outputId": "d584a380-23be-40fc-9da4-ab146376641e"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "dMU_ZXIvmFjD",
    "outputId": "c2848e61-bb6a-41a5-f3ec-d4a18496ff16"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(8)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
